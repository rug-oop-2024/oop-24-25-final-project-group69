<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>DSC-0001: Decision Tree Classifier &#8212; Final Project 2024 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=6fefd858"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Final Project Documentation" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="dsc-0001-decision-tree-classifier">
<h1>DSC-0001: Decision Tree Classifier<a class="headerlink" href="#dsc-0001-decision-tree-classifier" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-11-09
<strong>Decision</strong>: Implement a DecisionTreeClassifier model for classification tasks.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>:
The DecisionTreeClassifier was chosen due to its simplicity and flexibility, as it can handle mixed data types and has no assumptions about data distribution.</p>
<p><strong>Reason</strong>:
Decision Trees provide a good compromise between simplicity and functionality for machine learning tasks.</p>
<p><strong>Limitations</strong>:
Not as robust as other methods on complex datasets and sensitive to data imbalances.</p>
<p><strong>Alternatives</strong>:
Naive Bayes, Neural Networks</p>
<p>—</p>
</section>
<section id="dsc-0002-k-nearest-neighbors-knn">
<h1>DSC-0002: K-Nearest Neighbors (KNN)<a class="headerlink" href="#dsc-0002-k-nearest-neighbors-knn" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-11-09
<strong>Decision</strong>: Implement a K-Nearest Neighbors model for classification tasks.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>:
The project requires a simple, non-parametric classification algorithm capable of handling multi-class problems.</p>
<p><strong>Reason</strong>:
KNN works well for smaller datasets and does not make strong assumptions about the data distribution. It uses distance metrics to classify data points, which makes it versatile across different types of data.</p>
<p><strong>Limitations</strong>:
Computationally expensive for large datasets, as predictions involve comparing the input to all stored instances.</p>
<p><strong>Alternatives</strong>:
Support Vector Machines (SVM)</p>
<p>—</p>
</section>
<section id="dsc-0003-random-forest-classifier">
<h1>DSC-0003: Random Forest Classifier<a class="headerlink" href="#dsc-0003-random-forest-classifier" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-11-09
<strong>Decision</strong>: Implement a Random Forest Classifier model for classification tasks.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>:
The project requires a robust and versatile classification algorithm that can handle high-dimensional datasets, avoid overfitting, and perform well with minimal hyperparameter tuning.</p>
<p><strong>Reason</strong>:
Random Forest is an ensemble learning method that aggregates the predictions of multiple decision trees to improve classification performance. It is particularly effective in preventing overfitting, even with large and complex datasets.</p>
<p><strong>Limitations</strong>:
Prediction time can be slower compared to simpler models, as it involves aggregating results from multiple trees.</p>
<p><strong>Alternatives</strong>:
Decision Tree Classifier, Gradient Boosting Machines</p>
<p>—</p>
</section>
<section id="dsc-0004-ard-regression">
<h1>DSC-0004: ARD Regression<a class="headerlink" href="#dsc-0004-ard-regression" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-11-09
<strong>Decision</strong>: Implement an ARD Regression model for regression tasks.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>:
ARD Regression is ideal for high-dimensional data as it automatically identifies and discards irrelevant features, reducing overfitting.</p>
<p><strong>Reason</strong>:
ARD Regression uses Bayesian inference to assign relevance to each feature, providing a robust, interpretable model that automatically handles feature selection.</p>
<p><strong>Limitations</strong>:
Sensitive to outliers.</p>
<p><strong>Alternatives</strong>:
Linear Regression, Elastic Net, Lasso Regression</p>
<p>—</p>
</section>
<section id="dsc-0005-lasso-regression">
<h1>DSC-0005: Lasso Regression<a class="headerlink" href="#dsc-0005-lasso-regression" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-11-09
<strong>Decision</strong>: Use a Lasso Regression model for regression tasks.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>:
Lasso Regression helps with feature selection and regularization, addressing overfitting in high-dimensional datasets by shrinking some coefficients to zero.</p>
<p><strong>Reason</strong>:
Lasso is a linear model that performs both regularization and feature selection. By penalizing the absolute value of coefficients, it encourages sparsity, leading to simpler, more interpretable models.</p>
<p><strong>Limitations</strong>:
Can struggle with highly correlated features, as it may arbitrarily select one feature over another.</p>
<p><strong>Alternatives</strong>:
Ridge Regression, Elastic Net</p>
<p>—</p>
</section>
<section id="dsc-0006-ridge-regression">
<h1>DSC-0006: Ridge Regression<a class="headerlink" href="#dsc-0006-ridge-regression" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-11-09
<strong>Decision</strong>: Use a Ridge Regression model for regression tasks.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>:
Ridge Regression is used to prevent overfitting in linear regression models by adding an L2 penalty to the coefficients, making the model less sensitive to multicollinearity.</p>
<p><strong>Reason</strong>:
Ridge regression reduces the model’s complexity by penalizing large coefficients, thus improving the model’s robustness and accuracy.</p>
<p><strong>Limitations</strong>:
It can be ineffective when there are irrelevant features in the dataset.</p>
<p><strong>Alternatives</strong>:
Lasso Regression, Elastic Net</p>
</section>
<section id="dsc-0007-metrics-implementation">
<h1>DSC-0007: Metrics Implementation<a class="headerlink" href="#dsc-0007-metrics-implementation" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-10-20
<strong>Decision</strong>: Implement metrics for model evaluation.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>:
To evaluate the performance of machine learning models.</p>
<p><strong>Reason</strong>:
Each metric provides a quantitative measure of model accuracy, precision, or error.</p>
<p><strong>Limitations</strong>:
Some metrics, like R_square, assume specific data distributions.</p>
<p><strong>Alternatives</strong>:
Use external libraries such as scikit-learn.</p>
</section>
<section id="dsc-0008-artifacts-id">
<h1>DSC-0008: Artifacts ID<a class="headerlink" href="#dsc-0008-artifacts-id" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-10-25
<strong>Decision</strong>: Use a base64-encoded combination of asset_path and version to generate a unique artifact ID.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>: Using a combination of asset_path and version ensures that each artifact can be uniquely identified, even if artifacts share the same name but differ in version or location.</p>
<p><strong>Reason</strong>: Base 64 encoding provides uniqueness, non-colliding format, making retrieval and matching easier.</p>
<p><strong>Limitations</strong>: Base64 encoding increases the length of the ID, which may slightly impact storage.</p>
<p><strong>Alternatives</strong>: Concatenation without encoding, Hashing.</p>
</section>
<section id="dsc-0009-execute-method-pipeline">
<h1>DSC-0009: Execute Method Pipeline<a class="headerlink" href="#dsc-0009-execute-method-pipeline" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-10-25
<strong>Decision</strong>: Implement the execute method to run the complete ML pipeline.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>: To streamline and automate the machine learning workflow, ensuring consistent preprocessing, training, and evaluation.</p>
<dl class="simple">
<dt><strong>Reason</strong>: After evaluating on the test set, this method temporarily switches to evaluating on the training set to calculate training metrics, allowing a comparison between training and testing performance. The test data is then restored for any subsequent operations.</dt><dd><p>A unified method simplifies the pipeline’s operation, reduces manual errors, and provides a standard structure for metrics and predictions. (lines 228-290 in pipeline)</p>
</dd>
</dl>
<p><strong>Limitations</strong>: Metrics may not fully capture model behavior on unseen data if the dataset is small.</p>
<p><strong>Alternatives</strong>: Putting arguments in the evaluate function.</p>
</section>
<section id="dsc-0010-dataset-getter-and-setter-inside-pipeline-class">
<h1>DSC-0010: Dataset getter and setter inside pipeline class<a class="headerlink" href="#dsc-0010-dataset-getter-and-setter-inside-pipeline-class" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-10-25
<strong>Decision</strong>: Implement a getter and a setter.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>: In the deployment page, the dataset attribute had to be set after the pipeline was created.
It was necessary to validate the value of this dataset since the input and target features in the pipeline had to be the same as the ones in the uploaded dataset.</p>
<p><strong>Reason</strong>: Validation and leakage prevention needed.</p>
<p><strong>Alternatives</strong>: Reinstantiate the pipeline with the selected dataset.</p>
</section>
<section id="dsc-0011-split-values">
<h1>DSC-0011: Split values<a class="headerlink" href="#dsc-0011-split-values" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-10-25
<strong>Decision</strong>: What values the split can take.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>: The KNN k value is 5 by default. Giving that the minimum value for split was 0.01, there wouldn’t be enough neighbors for a prediction if a dataset had less than 500 samples.
Therefore, to prevent any problems, we made the minimum split value 0.2 and it increases by 0.05.</p>
<p><strong>Reason</strong>: The KNN model cannot give a prediction when the split is very small.</p>
<p><strong>Limitations</strong>:</p>
<p><strong>Alternatives</strong>: Add warning message specifically for KNN predictions, to encourage user to choose a larger split value.</p>
</section>
<section id="dsc-0012-dictionaries-with-artifact-names-as-keys-and-artficacts-as-values">
<h1>DSC-0012: Dictionaries with artifact names as keys and artficacts as values<a class="headerlink" href="#dsc-0012-dictionaries-with-artifact-names-as-keys-and-artficacts-as-values" title="Link to this heading">¶</a></h1>
<p><strong>Date</strong>: 2024-10-25
<strong>Decision</strong>: Create dictionaries with artifact names as keys and artficacts as values, in the Modelling and Deployment pages.
<strong>Status</strong>: Accepted</p>
<p><strong>Motivation</strong>: We wanted the user to be prompted with the artifacts name, not the artifacts themselves.</p>
<p><strong>Reason</strong>: For clarity reasons.</p>
<p><strong>Limitations</strong>: Storage, since dictionaries take up more storage.</p>
<p><strong>Alternatives</strong>: Tuples with the same functionality.</p>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Final Project</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Documentation Structure:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">DSC-0001: Decision Tree Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dsc-0002-k-nearest-neighbors-knn">DSC-0002: K-Nearest Neighbors (KNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dsc-0003-random-forest-classifier">DSC-0003: Random Forest Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dsc-0004-ard-regression">DSC-0004: ARD Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dsc-0005-lasso-regression">DSC-0005: Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dsc-0006-ridge-regression">DSC-0006: Ridge Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dsc-0007-metrics-implementation">DSC-0007: Metrics Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dsc-0008-artifacts-id">DSC-0008: Artifacts ID</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dsc-0009-execute-method-pipeline">DSC-0009: Execute Method Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dsc-0010-dataset-getter-and-setter-inside-pipeline-class">DSC-0010: Dataset getter and setter inside pipeline class</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dsc-0011-split-values">DSC-0011: Split values</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dsc-0012-dictionaries-with-artifact-names-as-keys-and-artficacts-as-values">DSC-0012: Dictionaries with artifact names as keys and artficacts as values</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Final Project Documentation</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Ionescu Alexia and Coja Ionut.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/docs.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>